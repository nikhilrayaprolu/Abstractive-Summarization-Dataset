{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 150000; we now have 150000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 150000 total words. Last word added: two-week-long\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Vocab at 0x7f4d350cbe10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"Summarization dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, file_name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_name (string): Path to the file with annotations.\n",
    "        \"\"\"\n",
    "        self.file_name = file\n",
    "        self.len_of_file = 287227\n",
    "        with open('filename.pickle', 'rb') as handle:\n",
    "            self.dataset =  pickle.load(handle)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.len_of_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'article': self.dataset[i].article, 'abstract': self.dataset[i].abstract }\n",
    "        return sample\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import struct\n",
    "import csv\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "# <s> and </s> are used in the data files to segment the abstracts into sentences. They don't receive vocab ids.\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "# Note: none of <s>, </s>, [PAD], [UNK], [START], [STOP] should appear in the vocab file.\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\"Vocabulary class for mapping between words and ids (integers)\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_file, max_size):\n",
    "        \"\"\"Creates a vocab of up to max_size words, reading from the vocab_file. If max_size is 0, reads the entire vocab file.\n",
    "\n",
    "        Args:\n",
    "            vocab_file: path to the vocab file, which is assumed to contain \"<word> <frequency>\" on each line, sorted with most frequent word first. This code doesn't actually use the frequencies, though.\n",
    "            max_size: integer. The maximum size of the resulting Vocabulary.\"\"\"\n",
    "        self._word_to_id = {}\n",
    "        self._id_to_word = {}\n",
    "        self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "        # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "        for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "            self._word_to_id[w] = self._count\n",
    "            self._id_to_word[self._count] = w\n",
    "            self._count += 1\n",
    "\n",
    "        # Read the vocab file and add words up to max_size\n",
    "        with open(vocab_file, 'r') as vocab_f:\n",
    "            for line in vocab_f:\n",
    "                pieces = line.split()\n",
    "                if len(pieces) != 2:\n",
    "                    print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line) \n",
    "                    continue\n",
    "                w = pieces[0]\n",
    "                if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "                    raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "                if w in self._word_to_id:\n",
    "                    raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "                self._word_to_id[w] = self._count\n",
    "                self._id_to_word[self._count] = w\n",
    "                self._count += 1\n",
    "                if max_size != 0 and self._count >= max_size:\n",
    "                    print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "                    break\n",
    "\n",
    "        print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "    def word2id(self, word):\n",
    "        \"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"\n",
    "        if word not in self._word_to_id:\n",
    "            return self._word_to_id[UNKNOWN_TOKEN]\n",
    "        return self._word_to_id[word]\n",
    "\n",
    "    def id2word(self, word_id):\n",
    "        \"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"\n",
    "        if word_id not in self._id_to_word:\n",
    "            raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "        return self._id_to_word[word_id]\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"Returns the total size of the vocabulary\"\"\"\n",
    "        return self._count\n",
    "\n",
    "    def write_metadata(self, fpath):\n",
    "        \"\"\"Writes metadata file for Tensorboard word embedding visualizer as described here:\n",
    "            https://www.tensorflow.org/get_started/embedding_viz\n",
    "\n",
    "        Args:\n",
    "            fpath: place to write the metadata file\n",
    "        \"\"\"\n",
    "        print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "        with open(fpath, \"w\") as f:\n",
    "            fieldnames = ['word']\n",
    "            writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "            for i in xrange(self.size()):\n",
    "                writer.writerow({\"word\": self._id_to_word[i]})\n",
    "Vocab('finished_files/vocab', 150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpython",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

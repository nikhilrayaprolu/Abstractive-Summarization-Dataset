{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. import packages\n",
    "import spacy\n",
    "import os\n",
    "from collections import Counter\n",
    "import glob\n",
    "\n",
    "from spacy import attrs\n",
    "import numpy as np\n",
    "vocab_size = 50000\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load nlp model & files to read\n",
    "nlp = spacy.load('en') # loads default English object\n",
    "cnn_dir = '../cnn_stories_tokenized/'\n",
    "cnn_pre_dir = '../cnn_stories_final/'\n",
    "file_list = [os.path.join(cnn_dir,file) for file in os.listdir(cnn_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch\n",
    "import glob\n",
    "from spacy import attrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_cnn(file_dir, nlp):\n",
    "\twith open(file_dir,encoding='utf-8') as f:\n",
    "\t\ttext = f.read()\n",
    "\t\ttext = text.lower()\n",
    "\t\ttext = text.replace('\\n\\n',' ')\n",
    "\t\ttext = text.split(\"@highlight\")\n",
    "\t\tbody = nlp(text[0])\n",
    "\t\tbody_words = [x.text for x in list(body)]\n",
    "\t\tsummaries = text[1:]\n",
    "\t\tsummaries = ' '.join([x+'.' for x in summaries])\n",
    "\t\tsummaries = nlp(summaries)\n",
    "\t\tsummary_words = [x.text for x in list(summaries)]\n",
    "\t\treturn body_words, summary_words\n",
    "\n",
    "def word_list_to_idx_list(word_list, word2idx, vocab_size):\n",
    "\tout = []\n",
    "\toov2idx = dict()\n",
    "\toov_words = []\n",
    "\tfor word in word_list:\n",
    "\t\ttry:\n",
    "\t\t\tout.append(word2idx[word])\n",
    "\t\texcept KeyError:\n",
    "\t\t\tif word not in oov2idx:\n",
    "\t\t\t\toov2idx[word]=vocab_size+len(oov2idx)\n",
    "\t\t\tout.append(oov2idx[word])\n",
    "\treturn out\n",
    "\n",
    "def calc_running_avg_loss(loss, running_avg_loss, step, decay=0.99):\n",
    "\tif running_avg_loss==0:\n",
    "\t\trunning_avg_loss = loss\n",
    "\telse:\n",
    "\t\trunning_avg_loss = running_avg_loss * decay + (1-decay) * loss\n",
    "\trunning_avg_loss = min(running_avg_loss,12) # clip\n",
    "\treturn running_avg_loss\n",
    "\n",
    "def to_cuda(item):\n",
    "\tif torch.cuda.is_available():\n",
    "\t\treturn item.cuda()\n",
    "\telse:\n",
    "\t\treturn item\n",
    "\n",
    "def num_to_var(item):\n",
    "\t# numpy array to Variable\n",
    "\tif item.dtype==int:\n",
    "\t\tout = Variable(torch.LongTensor(item))\n",
    "\telse:\n",
    "\t\tout = Variable(torch.Tensor(item))\n",
    "\treturn to_cuda(out)\n",
    "\t\t\n",
    "body_list = []\n",
    "summary_list = []\n",
    "counter = Counter()\n",
    "batch_no = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary created\n",
      "most common\n",
      "Vocabulary created from 1000/92579 files, top 32730 words saved\n",
      "summary created\n",
      "most common\n",
      "Vocabulary created from 2000/92579 files, top 45516 words saved\n",
      "summary created\n",
      "most common\n",
      "Vocabulary created from 3000/92579 files, top 50001 words saved\n",
      "summary created\n",
      "most common\n",
      "Vocabulary created from 4000/92579 files, top 50001 words saved\n",
      "summary created\n",
      "most common\n",
      "Vocabulary created from 5000/92579 files, top 50001 words saved\n",
      "0.995\r"
     ]
    }
   ],
   "source": [
    "while batch_no<len(file_list):\n",
    "    batch = file_list[batch_no:min(batch_no + batch_size,len(file_list))]\n",
    "    count = 0\n",
    "    for file in batch:\n",
    "        print(count/len(batch),end=\"\\r\")\n",
    "        count+=1\n",
    "        body_words, summary_words = parse_cnn(file,nlp)\n",
    "        body_list.extend(body_words)\n",
    "        summary_list.extend(summary_words)\n",
    "    print(\"summary created\")\n",
    "    c = Counter(body_list+summary_list)\n",
    "    counter = counter + c\n",
    "    vocab_list = counter.most_common(vocab_size)\n",
    "    print(\"most common\")\n",
    "    word2idx = dict()\n",
    "    word2idx['<PAD>']=0\n",
    "    word2idx['<S>']=1\n",
    "    word2idx['</S>']=2\n",
    "    word2idx['<UNK>']=3\n",
    "    idx2word = dict()\n",
    "    idx2word[0] = '<PAD>'\n",
    "    idx2word[1] = '<S>'\n",
    "    idx2word[2] = '</S>'\n",
    "    idx2word[3] = '<UNK>'\n",
    "    for i,(word,_) in enumerate(vocab_list):\n",
    "        if len(word2idx)>vocab_size:\n",
    "            break\n",
    "        word2idx[word] = i+4\n",
    "        idx2word[i+4] = word\n",
    "    np.save('word2idx.npy',word2idx)\n",
    "    np.save('idx2word.npy',idx2word)\n",
    "    batch_no+=batch_size\n",
    "    print(\"Vocabulary created from %d/%d files, top %d words saved\" \n",
    "          %(batch_no,len(file_list),len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 files processed so far\n",
      "2000 files processed so far\n",
      "3000 files processed so far\n",
      "4000 files processed so far\n",
      "5000 files processed so far\n",
      "6000 files processed so far\n",
      "7000 files processed so far\n",
      "8000 files processed so far\n",
      "9000 files processed so far\n",
      "10000 files processed so far\n",
      "11000 files processed so far\n",
      "12000 files processed so far\n",
      "13000 files processed so far\n",
      "14000 files processed so far\n",
      "15000 files processed so far\n",
      "16000 files processed so far\n",
      "17000 files processed so far\n",
      "18000 files processed so far\n",
      "19000 files processed so far\n",
      "20000 files processed so far\n",
      "21000 files processed so far\n",
      "22000 files processed so far\n",
      "23000 files processed so far\n",
      "24000 files processed so far\n",
      "25000 files processed so far\n",
      "26000 files processed so far\n",
      "27000 files processed so far\n",
      "28000 files processed so far\n",
      "29000 files processed so far\n",
      "30000 files processed so far\n",
      "31000 files processed so far\n",
      "32000 files processed so far\n",
      "33000 files processed so far\n",
      "34000 files processed so far\n",
      "35000 files processed so far\n",
      "36000 files processed so far\n",
      "37000 files processed so far\n",
      "38000 files processed so far\n",
      "39000 files processed so far\n",
      "40000 files processed so far\n",
      "41000 files processed so far\n",
      "42000 files processed so far\n",
      "43000 files processed so far\n",
      "44000 files processed so far\n",
      "45000 files processed so far\n",
      "46000 files processed so far\n",
      "47000 files processed so far\n",
      "48000 files processed so far\n",
      "49000 files processed so far\n",
      "50000 files processed so far\n",
      "51000 files processed so far\n",
      "52000 files processed so far\n",
      "53000 files processed so far\n",
      "54000 files processed so far\n",
      "55000 files processed so far\n",
      "56000 files processed so far\n",
      "57000 files processed so far\n",
      "58000 files processed so far\n",
      "59000 files processed so far\n",
      "60000 files processed so far\n",
      "61000 files processed so far\n",
      "62000 files processed so far\n",
      "63000 files processed so far\n",
      "64000 files processed so far\n",
      "65000 files processed so far\n",
      "66000 files processed so far\n",
      "67000 files processed so far\n",
      "68000 files processed so far\n",
      "69000 files processed so far\n",
      "70000 files processed so far\n",
      "71000 files processed so far\n",
      "72000 files processed so far\n",
      "73000 files processed so far\n",
      "74000 files processed so far\n",
      "75000 files processed so far\n",
      "76000 files processed so far\n",
      "77000 files processed so far\n",
      "78000 files processed so far\n",
      "79000 files processed so far\n",
      "80000 files processed so far\n",
      "81000 files processed so far\n",
      "82000 files processed so far\n",
      "83000 files processed so far\n",
      "84000 files processed so far\n",
      "85000 files processed so far\n",
      "86000 files processed so far\n",
      "87000 files processed so far\n",
      "88000 files processed so far\n",
      "89000 files processed so far\n",
      "90000 files processed so far\n",
      "91000 files processed so far\n",
      "92000 files processed so far\n"
     ]
    }
   ],
   "source": [
    "w2i = np.load('word2idx.npy').item()\n",
    "i2w = np.load('idx2word.npy').item()\n",
    "v = len(w2i)\n",
    "# 3. preprocess each document in CNN so that we get a form where a text is seen in vectors\n",
    "out_file_list = [os.path.join(cnn_pre_dir,file) for file in os.listdir(cnn_dir)]\n",
    "in_out_zip = zip(file_list, out_file_list)\n",
    "cnt = 0\n",
    "for in_file, out_file in in_out_zip:\n",
    "    body_words, summary_words = parse_cnn(in_file, nlp)\n",
    "    body_idx = word_list_to_idx_list(body_words, w2i, v)\n",
    "    body_idx = [str(x) for x in body_idx]\n",
    "    summary_idx = word_list_to_idx_list(summary_words,w2i,v)\n",
    "    summary_idx = [str(x) for x in summary_idx]\n",
    "    out = ' '.join(body_idx)+\"::\"+' '.join(summary_idx)\n",
    "    with open(out_file,'w') as f:\n",
    "        f.write(out)\n",
    "    cnt+=1\n",
    "    if cnt%1000==0:\n",
    "        print('%d files processed so far' %(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'a',\n",
       " 'south',\n",
       " 'korean',\n",
       " 'official',\n",
       " 'says',\n",
       " 'jill',\n",
       " 'kelley',\n",
       " \"'s\",\n",
       " 'use',\n",
       " 'of',\n",
       " 'her',\n",
       " 'honorary',\n",
       " 'title',\n",
       " 'was',\n",
       " '\"',\n",
       " 'not',\n",
       " 'suitable',\n",
       " '\"',\n",
       " '.',\n",
       " ' ',\n",
       " 'a',\n",
       " 'new',\n",
       " 'york',\n",
       " 'businessman',\n",
       " 'accused',\n",
       " 'her',\n",
       " 'of',\n",
       " 'using',\n",
       " 'that',\n",
       " 'designation',\n",
       " 'to',\n",
       " 'solicit',\n",
       " 'business',\n",
       " '.',\n",
       " ' ',\n",
       " 'kelley',\n",
       " \"'s\",\n",
       " 'complaint',\n",
       " 'about',\n",
       " 'harassing',\n",
       " 'e',\n",
       " '-',\n",
       " 'mails',\n",
       " 'led',\n",
       " 'to',\n",
       " 'the',\n",
       " 'resignation',\n",
       " 'of',\n",
       " 'cia',\n",
       " 'chief',\n",
       " 'david',\n",
       " 'petraeus',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'tv',\n",
       " 'personality',\n",
       " 'star',\n",
       " 'jones',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'heart',\n",
       " 'disease',\n",
       " 'in',\n",
       " '2010',\n",
       " '.',\n",
       " ' ',\n",
       " 'heart',\n",
       " 'disease',\n",
       " 'is',\n",
       " 'the',\n",
       " 'leading',\n",
       " 'cause',\n",
       " 'of',\n",
       " 'death',\n",
       " 'for',\n",
       " 'men',\n",
       " 'and',\n",
       " 'women',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'preventable',\n",
       " '.',\n",
       " ' ',\n",
       " 'february',\n",
       " 'is',\n",
       " 'american',\n",
       " 'heart',\n",
       " 'month',\n",
       " ',',\n",
       " 'and',\n",
       " 'friday',\n",
       " 'is',\n",
       " 'national',\n",
       " 'wear',\n",
       " 'red',\n",
       " 'day',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i,j in a:\n",
    "    tmp.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_list[0]) as f:\n",
    "    text = f.read()\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n\\n',' ')\n",
    "    text = text.replace('(cnn)','')\n",
    "    text = text.split(\"@highlight\")\n",
    "    body = text[0]\n",
    "    body_tokens = nlp(body)\n",
    "    summaries = text[1:]\n",
    "    summary_tokens = nlp(' '.join([x.strip()+'.' for x in summaries])+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2i = dict()\n",
    "w2i['<PAD>']=0\n",
    "w2i['<S>']=1\n",
    "w2i['</S>']=2\n",
    "\n",
    "i2w = dict()\n",
    "i2w[0]='<PAD>'\n",
    "i2w[1]='<S>'\n",
    "i2w[2]='</S>'\n",
    "\n",
    "for i,word in enumerate(word2idx):\n",
    "    if len(w2i)>50000:\n",
    "        break\n",
    "    w2i[word] = i+3\n",
    "    i2w[i+3] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_to_tokens(token_list,word2idx):\n",
    "    out = []\n",
    "    oov2idx = dict()\n",
    "    oov_idx = 0\n",
    "    for token in token_list:\n",
    "        word = token.text\n",
    "        try:\n",
    "            out.append(word2idx[word])\n",
    "        except KeyError:\n",
    "            if word not in oov2idx:\n",
    "                oov_idx+=1\n",
    "                oov2idx[word]=vocab_size+oov_idx\n",
    "            out.append(oov2idx[word])\n",
    "    return out, oov2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, oov2idx = nlp_to_tokens(list(body_tokens),word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx[l[1].text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(body)\n",
    "lst = list(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= []\n",
    "oov_dict = dict()\n",
    "for x in words:\n",
    "    try:\n",
    "        out.append(word2idx[x])\n",
    "    except KeyError:\n",
    "        oov_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx['oifdjherht']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common(300)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = list(np.arange(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while (i<10):\n",
    "    i+=1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "i = 0\n",
    "for file_name in file_list:\n",
    "    with open(file_name) as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n\\n',' ')\n",
    "        text = text.replace('(cnn)','')\n",
    "        text = text.split(\"@highlight\")\n",
    "        body = text[0]\n",
    "        doc = list(nlp(body))\n",
    "        word_list.extend([x.text for x in doc])\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c + Counter(['a','b','a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(list(set(word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "a = Variable(torch.LongTensor(np.arange(40).reshape(4,10)))\n",
    "emb = nn.Embedding(40,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(hidden_size=100,input_size=20, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.arange(24).reshape(4,6)\n",
    "A=A*(-1)\n",
    "A=A+15\n",
    "A = np.maximum(A,0)\n",
    "A = Variable(torch.LongTensor(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = A==0\n",
    "B.float().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Variable(torch.Tensor(1,4,100))\n",
    "out=lstm(emb(a[:,0].unsqueeze(1)), (c,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb(a).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(['a','a','a','a','a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch\n",
    "import glob\n",
    "from spacy import attrs\n",
    "\n",
    "\n",
    "word2idx = np.load('word2idx.npy').item()\n",
    "vocab_size = len(word2idx)\n",
    "batch_size = 1000\n",
    "\n",
    "nlp = spacy.load('en') # loads default English object\n",
    "cnn_dir = '/home/mjc/datasets/CNN_DailyMail/cnn/stories/'\n",
    "cnn_pre_dir = '/home/mjc/datasets/CNN_DailyMail/cnn/preprocessed_stories/'\n",
    "\n",
    "file_list = [os.path.join(cnn_dir,file) for file in os.listdir(cnn_dir)]\n",
    "total_files = len(file_list)\n",
    "files_read = 0\n",
    "count = 0\n",
    "for file in file_list[0:1]:\n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "        print(text)\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n\\n',' ')\n",
    "        text = text.replace('(cnn)','')\n",
    "        text = text.split(\"@highlight\")\n",
    "        body = text[0]\n",
    "        body_words = body.split(' ')\n",
    "        summaries = ' . '.join(text[1:])+' .'\n",
    "        summary_words = summaries.split(' ')\n",
    "        unique_words = list(set(body_words+summary_words))\n",
    "        temp_dict = dict()\n",
    "        oovs = 0\n",
    "        for w in unique_words:\n",
    "            try:\n",
    "                temp_dict[w] = word2idx[w]\n",
    "            except KeyError:\n",
    "                oovs+=1\n",
    "                temp_dict[w] = oovs+vocab_size\n",
    "        body_idx = [str(temp_dict[x]) for x in body_words]\n",
    "        summary_idx = [str(temp_dict[x]) for x in summary_words]\n",
    "        out = ' '.join(body_idx)+'::'+' '.join(summary_idx)\n",
    "        out_file = file.replace('/stories/','/preprocessed_stories/')\n",
    "    with open(out_file,'w') as f:\n",
    "        f.write(out)\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "\n",
    "\n",
    "# \t\tdoc = nlp(text)\n",
    "\n",
    "\n",
    "# counter = Counter()\n",
    "# while (files_read<total_files):\n",
    "#     word_list = []\n",
    "#     batch_files = file_list[files_read:min(files_read+1000,total_files)]\n",
    "#     for file_name in batch_files:\n",
    "#         with open(file_name) as f:\n",
    "#             text = f.read()\n",
    "#             text = text.lower()\n",
    "#             text = text.replace('\\n\\n',' ')\n",
    "#             text = text.replace('(cnn)','')\n",
    "#             text = text.split(\"@highlight\")\n",
    "#             body = text[0]\n",
    "#             doc = list(nlp(body))\n",
    "#             word_list.extend([x.text for x in doc])\n",
    "\n",
    "#     counter = counter + Counter(word_list)\n",
    "#     files_read+=len(batch_files)\n",
    "#     print(\"%d files read so far...\" % files_read)\n",
    "#     word2idx = {tup[0]: i for i,tup in enumerate(counter.most_common(vocab_size))}\n",
    "#     np.save('word2idx.npy',word2idx)\n",
    "# print(\"All merged!\")\n",
    "# word2idx = {tup[0]: i for i,tup in enumerate(counter.most_common(vocab_size))}\n",
    "# np.save('word2idx.npy',word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import sys\n",
    " \n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "def _binary_to_text():\n",
    "    reader = open('train.bin', 'rb')\n",
    "    writer = open('train.txt', 'w')\n",
    "    while True:\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes:\n",
    "            sys.stderr.write('Done reading\\n')\n",
    "            return\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        tf_example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        tf_example = example_pb2.Example.FromString(tf_example_str)\n",
    "        examples = []\n",
    "        for key in tf_example.features.feature:\n",
    "            examples.append('%s=%s' % (key, tf_example.features.feature[key].bytes_list.value[0]))\n",
    "        writer.write('%s\\n' % '\\t'.join(examples))\n",
    "    reader.close()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc2 in position 290: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2e77708eabb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_binary_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-9299cda03bb7>\u001b[0m in \u001b[0;36m_binary_to_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s=%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytes_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc2 in position 290: ordinal not in range(128)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class Batch(object):\n",
    "\tdef __init__(self, file_list, max_in_len, max_out_len, max_oovs):\n",
    "\t\tself.num_of_minibatch = len(file_list) # number of minibatches to read\n",
    "\t\tself.minibatch_count = 0 # how many minibatches read so far\n",
    "\t\tself.end_of_batch = False # whether we have reached the end of a batch\n",
    "\t\tself.max_in_len = max_in_len # maximum length of encoder input\n",
    "\t\tself.max_out_len = max_out_len # maximum length of decoded output\n",
    "\t\t# per minibatch\n",
    "\t\tself.minibatch_size = 0 # number of samples in a particula minibatch\n",
    "\t\tself.oov2idx_list = [] \n",
    "\t\tself.idx2oov_list = []\n",
    "\t\tself.input_lens = [] # lengths of encoding sequences within a batch \n",
    "\t\tself.output_lens = [] # lengths of decoding sequences within a batch \n",
    "\t\tself.max_oovs = max_oovs # max. number of OOVs possible\n",
    "\n",
    "\t# def get_minibatch(self,batch_list, vocab, deliminator=\":==:\"):\n",
    "\t# \t\"\"\"\n",
    "\t# \tArgs.\n",
    "\t# \tbatch_list : list of samples\n",
    "\t# \tvocab : a vocab object\n",
    "\t# \t\"\"\"\n",
    "\n",
    "\t# \t# releases a minibatch list & splits into stories and summaries\n",
    "\t# \tif self.end_of_batch == True:\n",
    "\t# \t\tprint (\"Trying to call from end of batch!\")\n",
    "\t# \t\treturn None\n",
    "\t# \tif self.sample_count+self.batch_size<self.len_samples:\n",
    "\t# \t\tout = batch_list[self.sample_count:self.sample_count+self.batch_size]\n",
    "\t# \t\tself.sample_count += self.batch_size\n",
    "\t# \telse: # end of batch\n",
    "\t# \t\tout = batch_list[self.sample_count:self.len_samples]\n",
    "\t# \t\tself.end_of_batch = True\n",
    "\n",
    "\t# \t# splits this list into story and summary\n",
    "\t# \tstories = []\n",
    "\t# \tsummaries = []\n",
    "\t# \tfor line in out:\n",
    "\t# \t\tstory, summary = line.split(deliminator)\n",
    "\t# \t\tstories.append(vocab.tokenize(story))\n",
    "\t# \t\tsummaries.append(vocab.tokenize(summary))\n",
    "\n",
    "\t# \tstories = self.match_minibatch_lengths()\n",
    "\n",
    "\t# \t# fixes lengths\n",
    "\t# \treturn stories, summaries\n",
    "\n",
    "\tdef process_minibatch(self,minibatch, vocab, deliminator=\":==:\"):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs.\n",
    "\t\tminibatch : text corpus or list\n",
    "\t\tvocab : a vocab object\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# splits this list into stories and summaries\n",
    "\t\tstories = []\n",
    "\t\tsummaries = []\n",
    "\t\tif type(minibatch)==str:\n",
    "\t\t\tminibatch = minibatch.split('\\n\\n')\n",
    "\t\tself.minibatch_size = len(minibatch)\n",
    "\t\t# for each sample in minibatch\n",
    "\t\tfor line in minibatch:\t\t\t\n",
    "\t\t\t# split each line into a story and a summary (word lists)\n",
    "\t\t\tstory, summary = line.split(deliminator)\n",
    "\t\t\tstory = vocab.tokenize(story)[:self.max_in_len]\n",
    "\t\t\tsummary = vocab.tokenize(summary)[:self.max_out_len-2]\n",
    "\t\t\tsummary = ['<SOS>']+summary+['<EOS>']\n",
    "\n",
    "\t\t\t# get all oov words from this sample and append them to list\n",
    "\t\t\toov2idx, idx2oov = vocab.create_oov_list(story+summary, self.max_oovs)\n",
    "\t\t\tself.oov2idx_list.append(oov2idx)\n",
    "\t\t\tself.idx2oov_list.append(idx2oov)\n",
    "\t\t\t# now change these into idx lists\n",
    "\t\t\tstory = vocab.word_list_to_idx_list(story,oov2idx)\n",
    "\t\t\tsummary = vocab.word_list_to_idx_list(summary,oov2idx)\n",
    "\t\t\t# append these to a list so that we have all stories/sums for a minibatch\n",
    "\t\t\tstories.append(story)\n",
    "\t\t\tsummaries.append(summary)\n",
    "\t\t# returns [b x max_seq] matrices which are indiced with numbers and zero padding\n",
    "\t\tstories, summaries = self.match_minibatch_lengths(vocab, stories, summaries)\n",
    "\t\treturn stories, summaries\n",
    "\n",
    "\t# initialize batch use at end of every epoch\n",
    "\tdef init_batch(self):\n",
    "\t\tself.batch_count = 0\n",
    "\t\tself.end_of_batch = False\n",
    "\n",
    "\t# initialize minibatch values at end of every minibatch\n",
    "\tdef init_minibatch(self):\n",
    "\t\tself.oov2idx_list = [] \n",
    "\t\tself.idx2oov_list = []\n",
    "\t\tself.input_lens = [] \n",
    "\t\tself.output_lens = []\n",
    "\t\tself.oov_len = 0\n",
    "\n",
    "\tdef match_minibatch_lengths(self, vocab, story_list, summary_list):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\tstory_list: list of story tokens\n",
    "\t\tsummary_list: list of summary tokens, <SOS> and <EOS> are not added so far\n",
    "\t\t\"\"\"\n",
    "\t\t# story_list = [x[:self.max_in_len] for x in story_list]\n",
    "\t\t# summary_list = [ [vocab.w2i['<SOS>']] + x[:self.max_out_len-2] +\n",
    "\t\t# [vocab.w2i['<EOS>']] for x in summary_list] # add <SOS> and <EOS>\n",
    "\t\t# get max lengths\n",
    "\t\tin_len = np.array([len(line) for line in story_list])\n",
    "\t\tout_len = np.array([len(line) for line in summary_list])\n",
    "\n",
    "\t\tmax_in = max(in_len)\n",
    "\t\tmax_out = max(out_len)\n",
    "\t\t# create numpy arrays to store indices\n",
    "\t\tstories_out = np.zeros([self.minibatch_size,max_in],dtype=int)\n",
    "\t\tsummaries_out = np.zeros([self.minibatch_size,max_out],dtype=int)\n",
    "\t\tfor b in range(self.minibatch_size):\n",
    "\t\t\tstories_out[b][:in_len[b]] = np.array(story_list[b])\n",
    "\t\t\tsummaries_out[b][:out_len[b]] = np.array(summary_list[b])\n",
    "\t\t# get summary lengths in descending order\n",
    "\t\tout_rev = out_len.argsort()[::-1]\n",
    "\t\t# return final values\n",
    "\t\tself.oov2idx_list = [self.oov2idx_list[i] for i in out_rev]\n",
    "\t\tself.idx2oov_list = [self.idx2oov_list[i] for i in out_rev]\n",
    "\t\tself.oov_len = max([len(x) for x in self.oov2idx_list])\n",
    "\t\tself.input_lens = in_len[out_rev]\n",
    "\t\tself.output_lens = out_len[out_rev]\n",
    "\t\treturn stories_out[out_rev], summaries_out[out_rev]\n",
    "\n",
    "\tdef unk_minibatch(self, minibatch, vocab):\n",
    "\t\t# for a numpy array minibatch, put all unks to zero\n",
    "\t\tunk_idx = vocab.w2i['<UNK>']\n",
    "\t\tvocab_idxs = np.array(minibatch<vocab.count,dtype=int)\n",
    "\t\toov_idxs = np.array(minibatch>=vocab.count,dtype=int) * unk_idx\n",
    "\t\tout = np.multiply(vocab_idxs, minibatch) # all OOV words are set to 0\n",
    "\t\treturn out + oov_idxs # OOV words are instead set to UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"Vocabulary class for mapping between words and ids (integers)\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_file, max_size):\n",
    "        \"\"\"Creates a vocab of up to max_size words, reading from the vocab_file. If max_size is 0, reads the entire vocab file.\n",
    "\n",
    "        Args:\n",
    "            vocab_file: path to the vocab file, which is assumed to contain \"<word> <frequency>\" on each line, sorted with most frequent word first. This code doesn't actually use the frequencies, though.\n",
    "            max_size: integer. The maximum size of the resulting Vocabulary.\"\"\"\n",
    "        self._word_to_id = {}\n",
    "        self._id_to_word = {}\n",
    "        self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "        # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "        for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "            self._word_to_id[w] = self._count\n",
    "            self._id_to_word[self._count] = w\n",
    "            self._count += 1\n",
    "\n",
    "        # Read the vocab file and add words up to max_size\n",
    "        with open(vocab_file, 'r') as vocab_f:\n",
    "            for line in vocab_f:\n",
    "                pieces = line.split()\n",
    "                if len(pieces) != 2:\n",
    "                    print 'Warning: incorrectly formatted line in vocabulary file: %s\\n' % line\n",
    "                    continue\n",
    "                w = pieces[0]\n",
    "                if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "                    raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "                if w in self._word_to_id:\n",
    "                    raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "                self._word_to_id[w] = self._count\n",
    "                self._id_to_word[self._count] = w\n",
    "                self._count += 1\n",
    "                if max_size != 0 and self._count >= max_size:\n",
    "                    print \"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count)\n",
    "                    break\n",
    "\n",
    "        print \"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1])\n",
    "\n",
    "    def word2id(self, word):\n",
    "        \"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"\n",
    "        if word not in self._word_to_id:\n",
    "            return self._word_to_id[UNKNOWN_TOKEN]\n",
    "        return self._word_to_id[word]\n",
    "\n",
    "    def id2word(self, word_id):\n",
    "        \"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"\n",
    "        if word_id not in self._id_to_word:\n",
    "            raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "        return self._id_to_word[word_id]\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"Returns the total size of the vocabulary\"\"\"\n",
    "        return self._count\n",
    "\n",
    "    def write_metadata(self, fpath):\n",
    "        \"\"\"Writes metadata file for Tensorboard word embedding visualizer as described here:\n",
    "            https://www.tensorflow.org/get_started/embedding_viz\n",
    "\n",
    "        Args:\n",
    "            fpath: place to write the metadata file\n",
    "        \"\"\"\n",
    "        print \"Writing word embedding metadata file to %s...\" % (fpath)\n",
    "        with open(fpath, \"w\") as f:\n",
    "            fieldnames = ['word']\n",
    "            writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "            for i in xrange(self.size()):\n",
    "                writer.writerow({\"word\": self._id_to_word[i]})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpython",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
